<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="">
  <meta property="og:title" content="MEDITITE"/>
  <meta property="og:description" content="Mindfulness Meditation and Respiration: Accelerometer-Based Respiration Rate and Mindfulness Progress Estimation to Enhance App Engagement and Mindfulness Skills"/>
  <meta property="og:url" content=""/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <!-- <meta property="og:image" content="static/image/your_banner_image.png" /> -->
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="MEDITITE">
  <meta name="twitter:description" content="Mindfulness Meditation and Respiration: Accelerometer-Based Respiration Rate and Mindfulness Progress Estimation to Enhance App Engagement and Mindfulness Skills">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Multimodal LLM">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>RAVEN</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Mindfulness Meditation and Respiration: Accelerometer-Based Respiration Rate and Mindfulness Progress Estimation to Enhance App Engagement and Mindfulness Skills</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://users.wpi.edu/~mkhan/" target="_blank">Mohammad Nur Hossain Khan<sup>1</sup></a>,</span>
                <span class="author-block">
                  <a href="https://www.cmu.edu/dietrich/psychology/directory/core-training-faculty/creswell-david.html" target="_blank">David Creswell</a>,</span>
                  <span class="author-block">
                    <a href="https://equahealth.io/" target="_blank">Jordan Albert<sup>2,3</sup></a>,</span>
                    <span class="author-block">
                      <a href="https://equahealth.io/" target="_blank">Patrick O'Connell<sup>2,3</sup></a>,</span>
                      <span class="author-block">
                        <a href="https://equahealth.io/" target="_blank">Shawn Fallon<sup>2,3</sup></a>,</span>
                        <span class="author-block">
                          <a href="https://equahealth.io/" target="_blank">Mathew Polowitz<sup>2,3</sup></a>,</span>
                          <span class="author-block">
                            <a href="https://orsonxu.com/" target="_blank">Xuhai "Orson" Xu<sup>4</sup></a>,</span>
                  <span class="author-block">
                    <a href="https://users.wpi.edu/~bislam/" target="_blank">Bashima Islam<sup>1</sup></a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Worcester Polytechnic Institute, <sup>2</sup>Carnegie Mellon University, <sup>3</sup>Equa Health Inc., <sup>4</sup>Columbia University</span><br>
                    <!-- <span class="author-block" style="color: red">INTERSPEECH, 2025</span> -->
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                      <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://drive.google.com/file/d/1CsO2DIHnFs9jYY5U2aTBtDXLEO9wGw8h/view?usp=sharing" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                          <span>Paper</span>
                        </a>
                      </span>

                      <span class="link-block">
                        <a href="https://drive.google.com/file/d/1CsO2DIHnFs9jYY5U2aTBtDXLEO9wGw8h/view?usp=sharing" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="ai ai-arxiv"></i>
                        </span>
                          <span>ArXiv</span>
                        </a>
                      </span>

                  <!-- Github link -->
                    <span class="link-block">
                        <a href="./index.html" target="_blank"
                            class="external-link button is-normal is-rounded is-dark">
                            <span class="icon">
                                <i class="fab fa-github"></i>
                            </span>
                            <span>Code <span style="color: yellowgreen">(Coming Soon!)</span></span>
                        </a>
                        </span>
                        </a>
                    </span>
                        <span class="link-block">
                        <a href="./index.html" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon" style="vertical-align: middle; font-size: 20px;">ðŸ¤—</span>
                        <span>Dataset <span style="color: yellowgreen">(Coming Soon!)</span></span>
                        </a>
                      </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Mindfulness training is widely recognized for its benefits in reducing depression, anxiety, and loneliness. 
            With the rise of smartphone-based mindfulness apps, digital meditation has become more accessible, but sustaining long-term user engagement remains a challenge. 
            This paper explores whether respiration biosignal feedback and mindfulness skill estimation enhance system usability and skill development. 
            We develop a smartphoneâ€™s accelerometer-based respiration tracking algorithm, eliminating the need for additional wearables. Unlike existing methods, our approach accurately captures slow breathing patterns typical of mindfulness meditation. 
            Additionally, we introduce the first quantitative framework to estimate mindfulness skillsâ€”concentration, sensory clarity, and equanimityâ€”based on accelerometer-derived respiration data. 
            We develop and test our algorithms on 261 mindfulness sessions in both controlled and real-world settings. 
            A user study comparing an experimental group receiving biosignal feedback with a control group using a standard app shows that respiration feedback enhances system usability. 
            Our respiration tracking model achieves a mean absolute error (MAE) of 1.6 breaths per minute, closely aligning with ground truth data, while our mindfulness skill estimation attains F1 scores of 80â€“84\% in tracking skill progression. 
            By integrating respiration tracking and mindfulness estimation into a commercial app, we demonstrate the potential of smartphone sensors to enhance digital mindfulness training..
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Overview</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
          <img class="equal-height-img" src="static/images/meditite/imwut_overview.png" alt="MY ALT TEXT"/>
          <h2 class="subtitle has-text-centered">
            We integrate a feedback module into a commercially available mindfulness meditation app, featuring a real-time respiration rate chart, respiration statistics, and mindfulness progress feedback. 
            This is achieved by developing a respiration rate tracking algorithm and a mindfulness skill change estimation algorithm. 
            To assess the impact of this feedback, we conducted a user study evaluating user satisfaction, mindfulness progression, and engagement.
          </h2>
        </div>

        <div class="item">
          <img class="equal-height-img" src="static/images/meditite/equa_modules.png" alt="MY ALT TEXT"/>
          <h2 class="subtitle has-text-centered">
            Screenshot of different modules deployed in the app. The training module consists of necessary instructions for mindfulness meditation training, and the assessment module captures the baseline mindfulness state before the session. 
            Finally, the feedback module provides the users with biosignal feedback such as a respiration chart, respiration statistics, and estimated mindfulness change.
          </h2>
        </div>


      </div>
    </div>
  </div>
</section>

<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Architecture</h2>
      <img src="./static/images/meditite/imwut_overview.pdf">
      <div class="content has-text-justified">
        Overview of <strong>RAVEN</strong>. Each modality (video, audio, sensor) is encoded using pretrained encoders and
        projected into a shared space. The <strong>QuART</strong> module performs query-conditioned token relevance scoring to align
        informative tokens across modalities. The figure also highlights the three-stage training pipeline for alignment-
        aware multi-modal reasoning.
      </div>
    </div>
  </div>
</section> -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Quantitative Results</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
          <img class="equal-height-img" src="static/images/meditite/mae_by_bpm.png" alt="MY ALT TEXT"/>
          <h2 class="subtitle has-text-centered">
            The mean absolute error for different respiration rates shows that the proposed algorithm performs consistently over a wide range of respiration rates, including low respiration rate zones with a lower MAE compared to the SOTA methods.
          </h2>
        </div>
        <div class="item">
          <img class="equal-height-img" src="static/images/meditite/mae_pcc_rr.png" alt="MY ALT TEXT"/>
          <h2 class="subtitle has-text-centered">
            Performance comparison between the proposed algorithm and baseline algorithms. The proposed algorithm demonstrates less MAE and high PCC compared to existing SOTA methods.
          </h2>
        </div>

        <div class="item">
          <img class="equal-height-img" src="static/images/meditite/result_modality.png" alt="MY ALT TEXT"/>
          <h2 class="subtitle has-text-centered">
            Performance comparison of smartphone accelerometer with estimated respiration for mindfulness change estimation. Using accelerometer data over raw respiration rate performs better in all three mindfulness change predictions.
          </h2>
        </div>

        <div class="item">
          <img class="equal-height-img" src="static/images/meditite/result_net.png" alt="MY ALT TEXT"/>
          <h2 class="subtitle has-text-centered">
            Performance comparison of various models for estimating change in mindfulness skills. Our proposed model demonstrated superior performance with 80\%-84\% F1 score, outperforming others and highlighting its effectiveness in capturing mindfulness skill progression.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End image carousel -->



<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">User Study</h2>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <div style="text-align: center;">
            <img src="static/images/meditite/user_study_1.png" alt="MY ALT TEXT"/>
        </div>
        <h2 class="subtitle has-text-centered">
          Impact of respiration biosignal feedback shows higher skill change and higher number of sessions with positive changes in the experimental group than the control group.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <div style="text-align: center;">
            <img src="static/images/meditite/user_study2.png" alt="MY ALT TEXT"/>
        </div>
        <h2 class="subtitle has-text-centered">
          Participants in the biosignal-augmented condition exhibited higher engagement with the app, completing more sessions on average and spending more time in meditation.
        </h2>
      </div>
  </div>
</div>
</div>
</section>




  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @article{biswas2025raven,
        title={RAVEN: Query-Guided Representation Alignment for Question Answering over Audio, Video, Embedded Sensors, and Natural Language},
        author={Biswas, Subrata and Khan, Mohammad Nur Hossain and Islam, Bashima},
        journal={arXiv preprint arXiv:2505.17114},
        year={2025}
        }
      </code></pre>
    </div>
  </section> -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from theÂ <a href="https://nerfies.github.io" target="_blank">Nerfies</a>Â project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>
  </body>
</html>
